{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newspaper scraping\n",
    "\n",
    "## Imposing  structure on data\n",
    "\n",
    "* Web scraping focuses on the transformation of unstructured data on the web\n",
    "* Typically in html format, into structured data that can be stored\n",
    "* And analyzed in a central local database or spreadsheat\n",
    "\n",
    "![](../images/web2.jpg)\n",
    "\n",
    "here we consider the UK BBC News website i.e https://www.bbc.co.uk/search?q=covid+19&page=1 and searched about covid 19 in a search box present on the top of the page shown in the picture.\n",
    "\n",
    "![](../images/web3.png)\n",
    "\n",
    "\n",
    "\n",
    "Let we write the script to scrape the news by giving keywords in our python\n",
    "\n",
    "## Import the packages\n",
    "\n",
    "here the the BBC news website had written using html scripts to extract the html scripts we have to install the BeautifulSoup and requests packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2fdaa7df37e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pages to get\n",
    " After searching of keywords the results shown in the number of pages\n",
    "\n",
    "![](../images/web4.png)\n",
    "\n",
    "so we have to intialize the variable to request the particular page and the result of the searching will be stored in the dataframe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagesToGet= 1\n",
    "resultframe=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the keyword to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the query: covid 19 vaccination\n",
      "covid+19+vaccination\n"
     ]
    }
   ],
   "source": [
    "key=input(\"Enter the query: \")\n",
    "key=key.replace(' ','+')\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting html script\n",
    "write the code to extract html script using class names mentioned in the bbc news website.\n",
    "\n",
    "![](../images/web5.png)\n",
    "\n",
    "and the result stored in the csv file named <b>\"search_res.csv\" </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "https://www.bbc.co.uk/search?q=covid+19+vaccination+article&page=1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for page in range(1,pagesToGet+1):\n",
    "    print('processing page :', page)\n",
    "    url = 'https://www.bbc.co.uk/search?q='+key+'+article&page='+str(page)\n",
    "    print(url)\n",
    "    page=requests.get(url)            \n",
    "    soup=BeautifulSoup(page.text,'html.parser')\n",
    "    frame=[]\n",
    "    links=soup.find('ul',attrs={'class':'css-1lb37cz-Stack e1y4nx260'}).find_all('li')\n",
    "    #links=soup.find_all('li',attrs={'class':'o-listicle__item'})\n",
    "    print(len(links))\n",
    "    filename=\"search_res.csv\"\n",
    "    f=open(filename,\"w\", encoding = 'utf-8')\n",
    "    headers=\"Statement,Link,Date\\n\"\n",
    "    f.write(headers)\n",
    "    \n",
    "    for j in links:\n",
    "      \n",
    "        Statement = j.find(\"div\",attrs={'class':'css-l100ew-PromoContentSummary e1f5wbog1'}).find('p',attrs={'class':'css-1uw1j0b-PromoHeadline e1f5wbog2'}).find('a',attrs={'class':'css-vh7bxp-PromoLink e1f5wbog6'}).text.strip()\n",
    "       \n",
    "        Link = j.find(\"p\",attrs={'class':'css-1uw1j0b-PromoHeadline e1f5wbog2'}).find('a')['href'].strip()\n",
    "        Date = j.find('span',attrs={'class':'css-1hizfh0-MetadataSnippet ecn1o5v0'}).text[8:].strip()\n",
    "       \n",
    "        frame.append((Statement,Link,Date))\n",
    "        f.write(Statement.replace(\",\",\"^\")+\",\"+Link+\",\"+Date.replace(\",\",\"^\")+\"\\n\")\n",
    "    resultframe.extend(frame)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result csv file\n",
    "\n",
    "here in a page there are 10 links are displayed. The title of the news related to keyword and published url and published date are stored in the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Statement  \\\n",
      "0  Covid: Biden vows 100m vaccinations for US in ...   \n",
      "1  Coronavirus in South Africa: Two-day-old baby ...   \n",
      "\n",
      "                                                Link         Date  \n",
      "0  https://www.bbc.co.uk/news/world-us-canada-552...   9 Dec 2020  \n",
      "1   https://www.bbc.co.uk/news/world-africa-52752334  21 May 2020  \n"
     ]
    }
   ],
   "source": [
    "data=pd.DataFrame(resultframe, columns=['Statement','Link','Date'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Article package\n",
    "\n",
    "using the article package, we can display the various properties of news article like title of the news, summary of the news, meta description etc., we can take the link of the above result i.e data DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://www.bbc.co.uk/news/world-us-canada-552...\n",
       "1     https://www.bbc.co.uk/news/world-africa-52752334\n",
       "Name: Link, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Link']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### installing the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (5.3.1)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (4.9.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (3.5)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (2.24.0)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (3.1.0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (7.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (4.5.2)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.3->newspaper3k) (1.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.47.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (0.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.25.9)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\mohanapriya\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = data['Link'][1] #for example take a first result link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply parsing to know the properties of news article easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_article = Article(url, language=\"en\") # en for English \n",
    "res_article.download()  #download an article\n",
    "res_article.parse() #To parse the article \n",
    "res_article.nlp() #To perform natural language processing ie..nlp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying the title of the news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Covid: Biden vows 100m vaccinations for US in first 100 days'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_article.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying the text of the news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"My first 100 days won\\'t end the Covid-19 virus. I can\\'t promise that. But we did not get into this mess quickly. We\\'re not going to get out of it quickly,\" he said at the event in Delaware, giving few details of how the largest vaccination programme in US history would be carried out.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using BeautifulSoup to extract entire article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.bbc.co.uk/news/uk-england-leeds-53394717'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=data['Link'][2]\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/web6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A bed factory has seen eight workers test positive for coronavirus - the third in a series of outbreaks at similar sites in West Yorkshire.\n",
      "Highgrove Beds in Liversedge ceased production as a safety precaution with all staff being offered tests.\n",
      "The outbreak follows cases at Deep Sleep Beds in Ossett and Dura Beds in Batley over the past month.\n",
      "There have also been cases of coronavirus reported at two meat factories in West Yorkshire.\n",
      "Rachel Spencer-Henshall, director of public health at Kirklees Council, warned factory workers of the risk of car sharing.\n",
      "She said: \"With the bed factories, it's less about the industry itself and more about working in a factory setting.\n",
      "\"What interests me a lot more is how people get to and from work, because actually you find a lot of people are car-sharing and in those scenarios you're in quite close contact with others for quite a long period of time, dependent on the commute.\"\n",
      "Latest news and stories from YorkshireDeep Sleep Beds in Ossett sees four casesDura Beds in Batley: Eight staff contract Covid-19\n",
      "Kirklees Council have been working with Highgrove Beds for about five days with an increase in cases seen over the weekend.\n",
      "The factory has been was inspected by Kirklees Council and \"presented high standards of infection control procedures and practices\".\n",
      "In a joint statement, the company, the council and Public Health England said the risk to local residents from the outbreak was very low.\n",
      "A SIMPLE GUIDE:  How do I protect myself?IMPACT: What the virus does to the bodyRECOVERY: How long does it take?LOCKDOWN: How can we lift restrictions?ENDGAME: How do we get out of this mess?\n",
      "Earlier this month, four workers at Deep Sleep Beds tested positive for Covid-19.\n",
      "On 1 July, it was revealed that eight staff at Dura Beds had contracted the virus.\n",
      "This comes as Forza Foods in Normanton reported 17 positive cases. \n",
      "Last month it was confirmed that 165 staff working at the Kober factory in Cleckheaton had tested positive for the virus.\n",
      "Why are there outbreaks in meat processing plants?Forza Foods open despite coronavirus cases'Secrecy' claims over Cleckheaton outbreak\n",
      "Ms Spencer-Henshall said that certain industries have seen increases in cases including meat processing. \n",
      "She added: \"Those factory conditions can be really attractive to the virus, particularly being a cold environment and quite loud so people are having to communicate and shout. \n",
      "\"We all know that singing is frowned upon at the moment and it's a similar thing in terms of projecting the virus.\"\n",
      "Follow BBC Yorkshire on Facebook, Twitter and Instagram. Send your story ideas to yorkslincs.news@bbc.co.uk or send video here.\n"
     ]
    }
   ],
   "source": [
    "page=requests.get(url)\n",
    "     \n",
    "soup=BeautifulSoup(page.text,'html.parser')\n",
    "frame=[]\n",
    "links=soup.find('article',attrs={'class':'css-5h7eao-ArticleWrapper e1nh2i2l0'}).find_all('div',attrs={'class':'css-uf6wea-RichTextComponentWrapper e1xue1i83'})\n",
    "#links=soup.find_all('li',attrs={'class':'o-listicle__item'})\n",
    "#print(\"l:\"+links)\n",
    "for i in links:\n",
    "    news=i.find('div',attrs={'class':'css-83cqas-RichTextContainer e5tfeyi2'}).text\n",
    "    print(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}